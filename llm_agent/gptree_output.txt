# Project Directory Structure:
.
├── .gptree_config
├── README.md
├── __init__.py
├── __manifest__.py
├── data/
├── models/
│   ├── __init__.py
│   ├── llm_model.py
│   ├── llm_provider.py
│   ├── llm_thread.py
│   ├── llm_tool.py
│   ├── llm_tool_record_retriever.py
│   └── llm_tool_server_action.py
├── security/
│   └── ir.model.access.csv
├── static/
│   └── src/
│       ├── components/
│       │   └── llm_chat_thread_header/
│       └── models/
│           ├── llm_chat.js
│           ├── llm_thread.js
│           └── llm_tool.js
└── views/
    ├── llm_thread_views.xml
    ├── llm_tool_server_action_views.xml
    └── llm_tool_views.xml

# BEGIN FILE CONTENTS

# File: static/src/models/llm_thread.js

/** @odoo-module **/

import { registerPatch } from "@mail/model/model_core";
import { attr, many } from "@mail/model/model_field";
import { clear } from "@mail/model/model_field_command";

registerPatch({
    name: "Thread",
    fields: {
        // Track selected tool IDs for this thread
        selectedToolIds: many("number"),
        
        // Computed field to get selected tools information
        selectedTools: many("LLMTool", {
            compute() {
                if (!this.selectedToolIds || !this.llmChat?.tools) {
                    return clear();
                }
                
                return this.llmChat.tools.filter(tool => 
                    this.selectedToolIds.includes(tool.id)
                );
            }
        }),
    },
    recordMethods: {
        /**
         * Extended method to include tools in settings updates
         * @override
         */
        async updateLLMChatThreadSettings({
            name,
            llmModelId,
            llmProviderId,
            toolIds,
        } = {}) {
            const values = {};

            // Only include name if it's a non-empty string
            if (typeof name === "string" && name.trim()) {
                values.name = name.trim();
            }

            // Only include model_id if it's a valid ID
            if (Number.isInteger(llmModelId) && llmModelId > 0) {
                values.model_id = llmModelId;
            } else if (this.llmModel?.id) {
                values.model_id = this.llmModel.id;
            }

            // Only include provider_id if it's a valid ID
            if (Number.isInteger(llmProviderId) && llmProviderId > 0) {
                values.provider_id = llmProviderId;
            } else if (this.llmModel?.llmProvider?.id) {
                values.provider_id = this.llmModel.llmProvider.id;
            }
            
            // Handle tools if provided
            if (Array.isArray(toolIds)) {
                values.tool_ids = [[6, 0, toolIds]];
            }

            // Only make the RPC call if there are values to update
            if (Object.keys(values).length > 0) {
                await this.messaging.rpc({
                    model: "llm.thread",
                    method: "write",
                    args: [[this.id], values],
                });
            }
        },
    },
});

# END FILE CONTENTS


# File: models/llm_thread.py

import json
import logging
from odoo import fields, models

_logger = logging.getLogger(__name__)

class LLMThread(models.Model):
    _inherit = "llm.thread"
    
    tool_ids = fields.Many2many(
        'llm.tool',
        string='Available Tools',
        help='Tools that can be used by the LLM in this thread'
    )
    
    def get_assistant_response(self, stream=True):
        """Get assistant response with tool handling"""
        try:
            messages = self.get_chat_messages()
            tool_ids = self.tool_ids.ids if self.tool_ids else None
            
            # Process response with possible tool calls
            response_generator = self._chat_with_tools(messages, tool_ids, stream)
            
            # Track for follow-up
            content = ""
            tool_messages = []
            assistant_message = None
            
            for response in response_generator:
                # Handle content
                if response.get("content") is not None:
                    content += response.get("content", "")
                    yield response
                
                # Handle tool calls
                if response.get("tool_call"):
                    tool_call = response.get("tool_call")
                    
                    # Create assistant message if not already created
                    if not assistant_message:
                        assistant_message = {
                            "role": "assistant",
                            "content": content if content else None,
                            "tool_calls": []
                        }
                    
                    # Add tool call to assistant message
                    assistant_message["tool_calls"].append({
                        "id": tool_call["id"],
                        "type": "function",
                        "function": {
                            "name": tool_call["function"]["name"],
                            "arguments": tool_call["function"]["arguments"]
                        }
                    })
                    
                    # Create tool message
                    tool_messages.append({
                        "role": "tool",
                        "tool_call_id": tool_call["id"],
                        "content": tool_call["result"]
                    })
                    
                    # Display raw tool output
                    raw_output = f"<strong>Tool:</strong> {tool_call['function']['name']}<br><br>"
                    raw_output += f"<strong>Arguments:</strong> <pre><code class='language-json'>{tool_call['function']['arguments']}</code></pre><br>"
                    raw_output += f"<strong>Result:</strong> <pre><code class='language-json'>{tool_call['result']}</code></pre><br>"
                    
                    yield {
                        "role": "assistant",
                        "content": raw_output
                    }
            
            # If we used tools, get interpretation
            if tool_messages and assistant_message:
                # Add messages to conversation
                updated_messages = messages.copy()
                updated_messages.append(assistant_message)
                
                # Add all tool messages
                for tm in tool_messages:
                    updated_messages.append(tm)
                
                # Get interpretation
                for interpretation in self._chat_with_tools(updated_messages, None, stream):
                    if interpretation.get("content") is not None:
                        yield interpretation
            
        except Exception as e:
            _logger.error("Error getting AI response: %s", str(e))
            yield {"error": str(e)}
    
    def _chat_with_tools(self, messages, tool_ids=None, stream=True):
        """Helper method to chat with tools"""
        return self.model_id.chat(
            messages=messages,
            stream=stream,
            tools=tool_ids
        )

# END FILE CONTENTS


# File: __init__.py

from . import models

# END FILE CONTENTS


# File: models/llm_tool_record_retriever.py

import json
import logging
from odoo import api, models, _

_logger = logging.getLogger(__name__)

class LLMToolRecordRetriever(models.Model):
    _inherit = "llm.tool"
    
    @api.model
    def _get_available_services(self):
        services = super()._get_available_services()
        return services + [
            ("odoo_record_retriever", "Odoo Record Retriever")
        ]
    
    # Implementation of the Odoo Record Retriever tool
    def odoo_record_retriever_execute(self, parameters):
        """Execute the Odoo Record Retriever tool"""
        _logger.info(f"Executing Odoo Record Retriever with parameters: {parameters}")
        
        model_name = parameters.get('model')
        domain = parameters.get('domain', [])
        fields = parameters.get('fields', [])
        limit = parameters.get('limit', 100)
        
        if not model_name:
            return {"error": "Model name is required"}
        
        try:
            model = self.env[model_name]
            
            # Validate domain structure
            if not isinstance(domain, list):
                return {"error": "Domain must be a list of criteria"}
            
            # Using search_read for efficiency
            if fields:
                result = model.search_read(domain=domain, fields=fields, limit=limit)
            else:
                records = model.search(domain=domain, limit=limit)
                result = records.read()
            
            # Convert to serializable format
            return json.loads(json.dumps(result, default=str))
        
        except KeyError:
            return {"error": f"Model '{model_name}' not found"}
        except Exception as e:
            _logger.exception(f"Error executing Odoo Record Retriever: {str(e)}")
            return {"error": str(e)}

# END FILE CONTENTS


# File: models/llm_model.py

from odoo import models


class LLMModel(models.Model):
    _inherit = "llm.model"

    def chat(self, messages, stream=False, tools=None, tool_choice="auto"):
        """Send chat messages using this model"""
        return self.provider_id.chat(messages, model=self, stream=stream, tools=tools, tool_choice=tool_choice)

    def embedding(self, texts):
        """Generate embeddings using this model"""
        return self.provider_id.embedding(texts, model=self)


# END FILE CONTENTS


# File: __manifest__.py

{
    "name": "LLM Agent",
    "version": "16.0.1.0.0",
    "category": "AI",
    "summary": "Tools for LLM models to interact with Odoo",
    "author": "Odoo",
    "website": "https://www.odoo.com",
    "license": "LGPL-3",
    "depends": ["base", "mail", "llm", "llm_thread", "llm_openai"],
    "data": [
        "security/ir.model.access.csv",
        "views/llm_thread_views.xml",
        "views/llm_tool_views.xml",
        "views/llm_tool_server_action_views.xml",
    ],
    "auto_install": False,
    "application": False,
    "installable": True,
}

# END FILE CONTENTS


# File: models/llm_tool_server_action.py

import json
import logging
from odoo import api, fields, models, _
from odoo.exceptions import UserError, AccessError

_logger = logging.getLogger(__name__)

class LLMToolServerAction(models.Model):
    _inherit = "llm.tool"
    
    # Add a field to store the bound server action
    server_action_id = fields.Many2one(
        'ir.actions.server', string='Bound Server Action',
        help='The specific server action this tool will execute'
    )
    
    @api.model
    def _get_available_services(self):
        services = super()._get_available_services()
        return services + [
            ("server_action", "Odoo Server Action")
        ]
    
    @api.onchange('server_action_id')
    def _onchange_server_action_id(self):
        """Update tool name and description based on the selected server action"""
        if self.server_action_id and self.service == 'server_action':
            # Generate a suitable name if not already set
            if not self.name or self.name.startswith('run_'):
                action_name = self.server_action_id.name.lower().replace(' ', '_')
                self.name = f"run_{action_name}"
            
            # Update description if not manually set
            if not self.description or 'This action works on the' in self.description:
                self.description = (
                    f"Run the '{self.server_action_id.name}' server action. "
                    f"This action works on the '{self.server_action_id.model_id.name}' model."
                )
            
            # Update schema based on the action type
            self._update_schema_for_server_action()
    
    def _update_schema_for_server_action(self):
        """Update the JSON schema based on the server action type"""
        if not self.server_action_id:
            return
            
        schema = {
            "type": "object",
            "properties": {}
        }
        
        # All server actions can accept a context
        schema["properties"]["context"] = {
            "type": "object",
            "description": "Additional context variables for the action"
        }
        
        # Add model-specific properties if it's not a multi-action
        if self.server_action_id.state != 'multi':
            # Add model and record_id fields
            schema["properties"]["record_id"] = {
                "type": "integer",
                "description": f"ID of the record to set as active_id"
            }
            
            # If it's a write or create action, it might need more specific fields
            if self.server_action_id.state in ['object_write', 'object_create']:
                # We might add specific fields here based on the model fields
                pass
        
        self.schema = json.dumps(schema, indent=2)
    
    # Implementation of the Odoo Server Action tool
    def server_action_execute(self, parameters):
        """Execute an Odoo Server Action tool
        
        Parameters:
            - record_id: Optional record ID to set as active_id
            - context: Optional context variables for the action
        """
        _logger.info(f"Executing Odoo Server Action with parameters: {parameters}")
        
        # Use the bound server action
        if not self.server_action_id:
            return {"error": "No server action is bound to this tool"}
            
        server_action = self.server_action_id
        record_id = parameters.get('record_id')
        context = parameters.get('context', {})
        
        try:
            # Check access rights
            if server_action.groups_id and not (server_action.groups_id & self.env.user.groups_id):
                return {"error": "You don't have enough access rights to run this action"}
            
            # Prepare execution context
            action_context = {}
            
            # Add active_id and active_model if provided
            if record_id:
                model = server_action.model_id.model
                
                # Validate record existence
                record = self.env[model].browse(record_id)
                if not record.exists():
                    return {"error": f"Record with ID {record_id} not found in model {model}"}
                
                # Check access rights on the record
                try:
                    record.check_access_rights('write')
                    record.check_access_rule('write')
                except AccessError:
                    return {"error": f"Access denied to record {record_id} of model {model}"}
                
                action_context.update({
                    'active_id': record_id,
                    'active_model': model,
                    'active_ids': [record_id],
                })
            
            # Add any additional context provided, sanitizing it first
            if context:
                safe_context = {k: v for k, v in context.items() 
                              if not k.startswith('_') and k not in ('uid', 'su')}
                action_context.update(safe_context)
            
            # Add audit logging
            _logger.info(
                f"LLM executing server action: id={server_action.id}, name='{server_action.name}', "
                f"model='{server_action.model_id.model}', record_id={record_id}, user={self.env.user.name}"
            )
            
            # Execute the server action with the prepared context
            result = server_action.with_context(**action_context).run()
            
            # Return the result (may be an action to execute)
            if result:
                # If result is an action dictionary, convert to JSON serializable
                return json.loads(json.dumps(result, default=str))
            else:
                return {"success": True, "message": f"Server action '{server_action.name}' executed successfully"}
                
        except Exception as e:
            _logger.exception(f"Error executing Server Action: {str(e)}")
            return {"error": str(e)}

# END FILE CONTENTS


# File: security/ir.model.access.csv

id,name,model_id:id,group_id:id,perm_read,perm_write,perm_create,perm_unlink
access_llm_tool_user,llm.tool.user,model_llm_tool,base.group_user,1,0,0,0
access_llm_tool_system,llm.tool.system,model_llm_tool,base.group_system,1,1,1,1

# END FILE CONTENTS


# File: README.md

# LLM Agent Module for Odoo 16

This module extends the LLM integration in Odoo by adding support for tools that can be used by LLM models to interact with the Odoo system.

## Features

- Define tools with JSON schema for parameters
- Uses the dispatch pattern for tool implementations
- Includes a record retriever tool for accessing Odoo data
- Implements OpenAI's tool calling API
- Supports streaming responses with tool execution

## Models

### llm.tool

Defines tools that LLM models can use to interact with Odoo:

- **name**: Tool name (must match what the LLM will call)
- **description**: Description of what the tool does
- **service**: The service that implements this tool
- **schema**: JSON Schema for the tool parameters
- **default**: Whether this tool should be included by default

### llm.thread (extended)

Extends the existing thread model to support tools:

- **tool_ids**: Many2many field linking to available tools for the thread

## Implementation

The module follows Odoo's best practices:

1. Uses the dispatch pattern for extensibility
2. Properly handles streaming responses
3. Provides good error handling and logging
4. Includes security access controls

## Usage Example

```python
# Get a thread with tools
thread = env['llm.thread'].browse(thread_id)

# Add a specific tool to the thread
record_retriever = env['llm.tool'].search([('name', '=', 'retrieve_records')])
thread.write({'tool_ids': [(4, record_retriever.id)]})

# Get a response that may use tools
for response in thread.get_assistant_response():
    # Handle the response
    print(response)
```

## Tool Implementation

Adding a new tool involves:

1. Creating a new tool record
2. Implementing the service method (e.g., `my_service_execute`)
3. Adding the service to the selection list

## Security

The module includes security access control rules:
- Regular users can read tools
- System administrators can create, update, and delete tools

# END FILE CONTENTS


# File: views/llm_tool_server_action_views.xml

<?xml version="1.0" encoding="utf-8"?>
<odoo>
    <!-- Enhance LLM Tool Form View with Server Action Field -->
    <record id="view_llm_tool_form_server_action" model="ir.ui.view">
        <field name="name">llm.tool.form.server.action</field>
        <field name="model">llm.tool</field>
        <field name="inherit_id" ref="view_llm_tool_form"/>
        <field name="arch" type="xml">
            <!-- Add server action field -->
            <xpath expr="//group[1]" position="after">
                <group string="Server Action" attrs="{'invisible': [('service', '!=', 'server_action')]}">
                    <field name="server_action_id" 
                           options="{'no_create': True}" 
                           domain="[('state', '!=', False)]"
                           attrs="{'required': [('service', '=', 'server_action')]}"/>
                </group>
            </xpath>
            
            <!-- Add a new page for server action tools -->
            <xpath expr="//notebook" position="inside">
                <page string="Server Action Settings" attrs="{'invisible': [('service', '!=', 'server_action')]}">
                    <group>
                        <group string="Audit">
                            <div colspan="2" class="text-muted">
                                All server action executions are logged in the server logs with user information.
                            </div>
                        </group>
                    </group>
                    <div class="alert alert-info" role="alert">
                        <p><strong>How this works:</strong></p>
                        <p>This tool is bound to a specific server action. When the LLM calls this tool, it will execute the bound server action.</p>
                        <p>The LLM can provide:</p>
                        <ul>
                            <li><strong>record_id</strong>: The ID of the record to operate on</li>
                            <li><strong>context</strong>: Additional context variables for the action</li>
                        </ul>
                        <p>The schema will be automatically updated based on the bound server action.</p>
                    </div>
                </page>
            </xpath>
        </field>
    </record>
</odoo>

# END FILE CONTENTS


# File: views/llm_tool_views.xml

<?xml version="1.0" encoding="utf-8"?>
<odoo>
    <!-- Tree View -->
    <record id="view_llm_tool_tree" model="ir.ui.view">
        <field name="name">llm.tool.tree</field>
        <field name="model">llm.tool</field>
        <field name="arch" type="xml">
            <tree string="LLM Tools">
                <field name="name"/>
                <field name="service"/>
                <field name="default"/>
                <field name="active"/>
            </tree>
        </field>
    </record>

    <!-- Form View -->
    <record id="view_llm_tool_form" model="ir.ui.view">
        <field name="name">llm.tool.form</field>
        <field name="model">llm.tool</field>
        <field name="arch" type="xml">
            <form string="LLM Tool">
                <sheet>
                    <div class="oe_button_box" name="button_box">
                        <button name="toggle_active" type="object" class="oe_stat_button" icon="fa-archive">
                            <field name="active" widget="boolean_button"/>
                        </button>
                    </div>
                    <div class="oe_title">
                        <label for="name" class="oe_edit_only"/>
                        <h1><field name="name" placeholder="e.g. retrieve_records"/></h1>
                    </div>
                    <group>
                        <group>
                            <field name="service"/>
                            <field name="default"/>
                        </group>
                    </group>
                    <notebook>
                        <page string="Description">
                            <field name="description" placeholder="Describe what this tool does and when to use it"/>
                        </page>
                        <page string="Schema">
                            <field name="schema" widget="text" placeholder="{...}"/>
                        </page>
                    </notebook>
                </sheet>
                <div class="oe_chatter">
                    <field name="message_follower_ids"/>
                    <field name="message_ids"/>
                </div>
            </form>
        </field>
    </record>

    <!-- Search View -->
    <record id="view_llm_tool_search" model="ir.ui.view">
        <field name="name">llm.tool.search</field>
        <field name="model">llm.tool</field>
        <field name="arch" type="xml">
            <search string="Search LLM Tools">
                <field name="name"/>
                <field name="service"/>
                <filter string="Default Tools" name="default" domain="[('default', '=', True)]"/>
                <filter string="Active" name="active" domain="[('active', '=', True)]"/>
                <group expand="0" string="Group By">
                    <filter string="Service" name="group_by_service" context="{'group_by': 'service'}"/>
                </group>
            </search>
        </field>
    </record>

    <!-- Action -->
    <record id="action_llm_tool" model="ir.actions.act_window">
        <field name="name">LLM Tools</field>
        <field name="res_model">llm.tool</field>
        <field name="view_mode">tree,form</field>
        <field name="context">{'search_default_active': 1}</field>
        <field name="help" type="html">
            <p class="o_view_nocontent_smiling_face">
                Create a new LLM Tool
            </p>
            <p>
                Define tools that LLM models can use to interact with Odoo.
            </p>
        </field>
    </record>

    <!-- Menu Item -->
    <menuitem id="menu_llm_tool"
              name="Tools"
              parent="llm.menu_llm_config"
              action="action_llm_tool"
              sequence="20"/>
</odoo>

# END FILE CONTENTS


# File: views/llm_thread_views.xml

<?xml version="1.0" encoding="utf-8"?>
<odoo>
    <!-- Inherit LLM Thread Form View -->
    <record id="view_llm_thread_form_inherit_tools" model="ir.ui.view">
        <field name="name">llm.thread.form.inherit.tools</field>
        <field name="model">llm.thread</field>
        <field name="inherit_id" ref="llm_thread.llm_thread_view_form"/>
        <field name="arch" type="xml">
            <!-- Add tools field after model_id -->
            <xpath expr="//field[@name='model_id']" position="after">
                <field name="tool_ids" widget="many2many_tags" options="{'no_create': True}"/>
            </xpath>
        </field>
    </record>
</odoo>

# END FILE CONTENTS


# File: static/src/models/llm_tool.js

/** @odoo-module **/

import { registerModel } from "@mail/model/model_core";
import { attr } from "@mail/model/model_field";

registerModel({
    name: "LLMTool",
    fields: {
        id: attr({
            identifying: true,
        }),
        name: attr({
            required: true,
        }),
    },
});

# END FILE CONTENTS


# File: .gptree_config

# GPTree Local Config
version: 1

# Whether to use .gitignore
useGitIgnore: true
# File types to include (e.g., .py,.js)
includeFileTypes: *
# File types to exclude when includeFileTypes is '*'
excludeFileTypes: 
# Output file name
outputFile: gptree_output.txt
# Whether to output the file locally or relative to the project directory
outputFileLocally: true
# Whether to copy the output to the clipboard
copyToClipboard: false
# Whether to use safe mode (prevent overly large files from being combined)
safeMode: true
# Whether to store the files chosen in the config file (--save, -s)
storeFilesChosen: true
# Previously selected files (when using the -s or --save flag previously)
previousFiles: 


# END FILE CONTENTS


# File: models/__init__.py

from . import llm_thread
from . import llm_tool
from . import llm_provider
from . import llm_tool_record_retriever
from . import llm_tool_server_action
from . import llm_model

# END FILE CONTENTS


# File: models/llm_provider.py

import json
import logging
from odoo import api, fields, models, _

_logger = logging.getLogger(__name__)

class LLMProvider(models.Model):
    _inherit = "llm.provider"
    
    def get_available_tools(self, tool_ids=None):
        """Get available tools for this provider
        
        Args:
            tool_ids: Optional specific tool ids to include
            
        Returns:
            List of tool definitions in the format expected by the provider
        """
        domain = [('active', '=', True)]
        
        if tool_ids:
            domain.append(('id', 'in', tool_ids))
        else:
            # Include default tools if no specific tools requested
            domain.append(('default', '=', True))
            
        tools = self.env['llm.tool'].search(domain)
        return tools

    def format_tools_for_provider(self, tools):
        """Format tools for the specific provider"""
        return self._dispatch("format_tools", tools)
        
    # OpenAI specific implementation
    def openai_format_tools(self, tools):
        """Format tools for OpenAI"""
        return [tool.to_tool_definition() for tool in tools]
        
    def openai_chat(self, messages, model=None, stream=False, tools=None, tool_choice="auto"):
        """Send chat messages using OpenAI with tools support"""
        model = self.get_model(model, "chat")
        
        # Prepare request parameters
        params = {
            "model": model.name,
            "messages": messages,
            "stream": stream,
        }
        
        # Add tools if specified
        if tools:
            tool_objects = self.get_available_tools(tools)
            formatted_tools = self.format_tools_for_provider(tool_objects)
            if formatted_tools:
                params["tools"] = formatted_tools
                params["tool_choice"] = tool_choice
        
        response = self.client.chat.completions.create(**params)
        
        if not stream:
            message = {
                "role": response.choices[0].message.role,
                "content": response.choices[0].message.content or "",  # Handle None content
            }
            
            # Handle tool calls if present
            if hasattr(response.choices[0].message, 'tool_calls') and response.choices[0].message.tool_calls:
                message["tool_calls"] = []
                
                for tool_call in response.choices[0].message.tool_calls:
                    # Find the tool
                    tool = self.env['llm.tool'].search([('name', '=', tool_call.function.name)], limit=1)
                    
                    if tool:
                        # Execute the tool
                        try:
                            arguments = json.loads(tool_call.function.arguments)
                            result = tool.execute(arguments)
                            
                            # Add tool call and result to message
                            message["tool_calls"].append({
                                "id": tool_call.id,
                                "type": "function",
                                "function": {
                                    "name": tool_call.function.name,
                                    "arguments": tool_call.function.arguments,
                                },
                                "result": json.dumps(result)
                            })
                        except Exception as e:
                            _logger.exception(f"Error executing tool {tool.name}: {str(e)}")
                            message["tool_calls"].append({
                                "id": tool_call.id,
                                "type": "function",
                                "function": {
                                    "name": tool_call.function.name,
                                    "arguments": tool_call.function.arguments,
                                },
                                "result": json.dumps({"error": str(e)})
                            })
                    else:
                        _logger.error(f"Tool {tool_call.function.name} not found")
            
            yield message
        else:
            current_tool_call = None
            tool_call_chunks = {}
            
            for chunk in response:
                delta = chunk.choices[0].delta
                
                # Handle normal content
                if hasattr(delta, 'content') and delta.content is not None:
                    yield {
                        "role": "assistant",
                        "content": delta.content,
                    }
                
                # Handle streaming tool calls
                if hasattr(delta, 'tool_calls') and delta.tool_calls:
                    _logger.info(f"Received tool call chunk: {delta.tool_calls}")
                    
                    for tool_call_chunk in delta.tool_calls:
                        index = tool_call_chunk.index
                        _logger.info(f"Processing tool call chunk with index: {index}")
                        
                        # Initialize tool call data if it's a new one
                        if index not in tool_call_chunks:
                            _logger.info(f"Initializing new tool call with index: {index}")
                            tool_call_chunks[index] = {
                                "id": tool_call_chunk.id,
                                "type": "function",
                                "function": {
                                    "name": "",
                                    "arguments": ""
                                }
                            }
                        
                        # First chunk typically contains id, name and type
                        if tool_call_chunk.id:
                            tool_call_chunks[index]["id"] = tool_call_chunk.id
                            _logger.info(f"Setting tool call ID: {tool_call_chunk.id}")
                        
                        if tool_call_chunk.type:
                            tool_call_chunks[index]["type"] = tool_call_chunk.type
                        
                        # Update function name if present
                        if (hasattr(tool_call_chunk, 'function') and 
                            hasattr(tool_call_chunk.function, 'name') and 
                            tool_call_chunk.function.name):
                            
                            tool_call_chunks[index]["function"]["name"] = tool_call_chunk.function.name
                            _logger.info(f"Setting tool name: '{tool_call_chunk.function.name}'")
                        
                        # Update arguments if present - this continues across multiple chunks
                        if (hasattr(tool_call_chunk, 'function') and 
                            hasattr(tool_call_chunk.function, 'arguments') and 
                            tool_call_chunk.function.arguments is not None):
                            
                            arg_chunk = tool_call_chunk.function.arguments
                            tool_call_chunks[index]["function"]["arguments"] += arg_chunk
                            _logger.info(f"Added argument chunk: '{arg_chunk}' to tool call {index}")
                            _logger.info(f"Current arguments: '{tool_call_chunks[index]['function']['arguments']}'")
                            
                        # Check if we received a complete JSON object - indicating arguments are complete
                        current_args = tool_call_chunks[index]["function"]["arguments"]
                        
                        if current_args and current_args.endswith('}'):
                            try:
                                # Validate JSON is complete by parsing it
                                json.loads(current_args)
                                
                                # Find and execute the tool
                                tool_name = tool_call_chunks[index]["function"]["name"]
                                _logger.info(f"Arguments complete. Looking for tool: '{tool_name}'")
                                
                                if not tool_name:
                                    _logger.warning(f"Empty tool name for index: {index}")
                                    continue
                                    
                                tool = self.env['llm.tool'].search([('name', '=', tool_name)], limit=1)
                                _logger.info(f"Tool search result: {tool}, tool.id: {tool.id if tool else 'Not found'}")
                                
                                if tool:
                                    _logger.info(f"Executing tool '{tool_name}' with arguments: {current_args}")
                                    arguments = json.loads(current_args)
                                    result = tool.execute(arguments)
                                    
                                    # Add result to tool call data
                                    tool_call_chunks[index]["result"] = json.dumps(result)
                                    
                                    # Yield the tool call with result
                                    yield {
                                        "role": "assistant",
                                        "tool_call": tool_call_chunks[index]
                                    }
                                else:
                                    _logger.error(f"Tool '{tool_name}' not found")
                            except json.JSONDecodeError:
                                # JSON not complete yet, continue accumulating
                                _logger.info("JSON arguments incomplete, continuing to accumulate")
                            except Exception as e:
                                _logger.exception(f"Error executing tool: {str(e)}")
                                tool_call_chunks[index]["result"] = json.dumps({"error": str(e)})
                                
                                yield {
                                    "role": "assistant",
                                    "tool_call": tool_call_chunks[index]
                                }
                        
    def chat(self, messages, model=None, stream=False, tools=None, tool_choice="auto"):
        """Send chat messages using this provider"""
        return self._dispatch("chat", messages, model=model, stream=stream, tools=tools, tool_choice=tool_choice)

# END FILE CONTENTS


# File: static/src/models/llm_chat.js

/** @odoo-module **/

import { registerPatch } from "@mail/model/model_core";
import { many } from "@mail/model/model_field";

registerPatch({
    name: "LLMChat",
    fields: {
        tools: many("LLMTool"),
    },
    recordMethods: {
        /**
         * Load tools from the server
         */
        async loadTools() {
            try {
                const result = await this.messaging.rpc({
                    model: "llm.tool",
                    method: "search_read",
                    kwargs: {
                        domain: [["active", "=", true]],
                        fields: ["name", "id"],
                    },
                });

                const toolData = result.map((tool) => ({
                    id: tool.id,
                    name: tool.name,
                }));

                this.update({ tools: toolData });
            } catch (error) {
                console.error("Error loading tools:", error);
                return [];
            }
        },
        
        /**
         * Extended version of ensureThread to load tools
         * @override
         */
        async ensureThread(options = {}) {
            // Call the original method first
            const thread = await this._super(options);
            
            // Load tools if not already loaded
            if (!this.tools || this.tools.length === 0) {
                await this.loadTools();
            }
            
            return thread;
        },
        
        /**
         * Extended version of loadThreads to include tool_ids
         * @override
         */
        async loadThreads() {
            const result = await this.messaging.rpc({
                model: "llm.thread",
                method: "search_read",
                kwargs: {
                    domain: [["create_uid", "=", this.env.services.user.userId]],
                    fields: [
                        "name",
                        "message_ids",
                        "create_uid",
                        "create_date",
                        "write_date",
                        "model_id",
                        "provider_id",
                        "related_thread_model",
                        "related_thread_id",
                        "tool_ids",
                    ],
                    order: "write_date desc",
                },
            });

            const threadData = result.map((thread) => ({
                id: thread.id,
                model: "llm.thread",
                name: thread.name,
                message_needaction_counter: 0,
                creator: thread.create_uid ? { id: thread.create_uid } : undefined,
                isServerPinned: true,
                updatedAt: thread.write_date,
                relatedThreadModel: thread.related_thread_model,
                relatedThreadId: thread.related_thread_id,
                selectedToolIds: thread.tool_ids || [],
                llmModel: thread.model_id
                    ? {
                        id: thread.model_id[0],
                        name: thread.model_id[1],
                        llmProvider: {
                            id: thread.provider_id[0],
                            name: thread.provider_id[1],
                        },
                    }
                    : undefined,
            }));

            this.update({ threads: threadData });
        },
        /**
         * Extended version of loadThreads to include tool_ids
         * @override
         */
        async initializeLLMChat(action, initActiveId) {
            this.update({
              llmChatView: {
                actionId: action.id,
              },
              initActiveId,
            });
      
            // Wait for messaging to be initialized
            await this.messaging.initializedPromise;
            await this.loadLLMModels();
            // Load threads first
            await this.loadThreads();
            await this.loadTools();
      
            // Then handle initial thread
            if (!this.isInitThreadHandled) {
              this.update({ isInitThreadHandled: true });
              if (!this.activeThread) {
                this.openInitThread();
              }
            }
          }
    },
});

# END FILE CONTENTS


# File: models/llm_tool.py

import json
import logging
from odoo import api, fields, models, _
from odoo.exceptions import UserError

_logger = logging.getLogger(__name__)

class LLMTool(models.Model):
    _name = "llm.tool"
    _description = "LLM Tool"
    _inherit = ["mail.thread"]

    name = fields.Char(required=True, tracking=True)
    description = fields.Text(required=True, tracking=True, 
                             help="Description of what the tool does. This will be sent to the LLM.")
    service = fields.Selection(
        selection=lambda self: self._selection_service(),
        required=True,
        help="The service that implements this tool",
    )
    active = fields.Boolean(default=True)
    schema = fields.Text(
        help="JSON Schema for the tool parameters in JSON format"
    )
    default = fields.Boolean(
        default=False,
        help="Set to true if this is a default tool to be included in all LLM requests"
    )
    
    def _dispatch(self, method, *args, **kwargs):
        """Dispatch method call to appropriate service implementation"""
        if not self.service:
            raise UserError(_("Tool service not configured"))

        service_method = f"{self.service}_{method}"
        if not hasattr(self, service_method):
            raise NotImplementedError(
                _("Method %s not implemented for service %s") % (method, self.service)
            )

        return getattr(self, service_method)(*args, **kwargs)
    
    @api.model
    def _selection_service(self):
        """Get all available services from tool implementations"""
        services = []
        for service in self._get_available_services():
            services.append(service)
        return services
    
    @api.model
    def _get_available_services(self):
        """Hook method for registering tool services"""
        return []
    
    def execute(self, parameters):
        """Execute the tool with the given parameters"""
        return self._dispatch("execute", parameters)
    
    def to_tool_definition(self):
        """Convert tool to OpenAI compatible tool definition"""
        return {
            "type": "function",
            "function": {
                "name": self.name,
                "description": self.description,
                "parameters": json.loads(self.schema),
            }
        }

# END FILE CONTENTS
